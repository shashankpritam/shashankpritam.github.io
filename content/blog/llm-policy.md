---
title: "LLM Policy"
date: 2025-04-06T16:58:20-05:00
draft: true
---

As a person who loves trying out new technologies and tools, I must say that I never imagined something like the large language models (LLMs) to exist in this lifetime, even though I have been following the development since the GPT2 era. They are truly remarkable and they already have a huge impact in the world. For example, writing regex and bash one-liners is surprisingly easy with LLMs. There are now, agents or agentic tools (for example - Cline) that can be incorporated into existing workflow and tools like VSCode. This has led to a pheonomena of "Vibe Coding"; which I find -- interesting.

Last week, I realized that I was "Vibe Coding" without even knowing it. It started with a simple task of generating documentation, writing CLI command parser, and debugging. Then I moved to writing a module for a task - initially this was a pleasant experience and made me feel truly productive. But then came the LLM fatigue. I didn't "know" the code - it was not mine. One big problem with the 'agent' is that they do have a brain of their own and without asking, based on assumption they'd generate code/content to fill in the information they lack. So, my job became - as a maintainer - of a project that I didn't even know about. It was quite frustrating. I had to spend a lot of time debugging and understanding the code that was generated. I realized that I was spending more time fixing the code than actually writing it. This made me question the whole process of using LLMs for coding.

There is a range of productivity and utility of

